{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Data Cleaning & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Import Libraries](#Import-Libraries)**.  \n",
    "    \n",
    "- **[Data Cleaning & EDA](#Data-Cleaning-Exploratory-Data-Analysis)**.  \n",
    "\n",
    "- **[Model Preparation](#Model-Preparation)**.  \n",
    "\n",
    "   - **[Baseline Model](#Baseline-Model)**. \n",
    "   - **[Pipeline Models](#Pipeline-Models)**. \n",
    "   - **[Logistic Regression Models](#Logistic-Regression-Models)**.  \n",
    "   - **[Naive Bayes Models](#Naive-Bayes-Models)**.  \n",
    "   - **[Random Forest](#Random-Forest)**.  \n",
    "   - **[Extra Trees](#Extra-Trees)**.  \n",
    "   \n",
    "- **[Model Evaluation](#Model-Evaluation)**.  \n",
    "\n",
    "- **[Conclusions and Recommendations](#Conclusions-and-Recommendations)**.  \n",
    "\n",
    "- **[References](#References)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_data = pd.read_csv('../data/result1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1724, 32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pets_data = pd.read_csv('../data/result2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2959, 32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pets_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of classes are similar in both data. There is no need to stratify or correct for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_data = pd.read_csv('../data/combined_db.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type.1</th>\n",
       "      <th>...</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>locked</th>\n",
       "      <th>media_only</th>\n",
       "      <th>over_18</th>\n",
       "      <th>pinned</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>purehoopla</td>\n",
       "      <td>1584915670</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1584915676</td>\n",
       "      <td>1</td>\n",
       "      <td>6852</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>Renegade626</td>\n",
       "      <td>1584933321</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1584933322</td>\n",
       "      <td>1</td>\n",
       "      <td>6855</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>mytrendybabystore</td>\n",
       "      <td>1584983015</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1584983020</td>\n",
       "      <td>1</td>\n",
       "      <td>6856</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>mytrendybabystore</td>\n",
       "      <td>1584988382</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1584988392</td>\n",
       "      <td>1</td>\n",
       "      <td>6857</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>kellimoxie</td>\n",
       "      <td>1585016982</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1585016988</td>\n",
       "      <td>1</td>\n",
       "      <td>6860</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit subreddit_type             author  created_utc  \\\n",
       "0      baby         public         purehoopla   1584915670   \n",
       "1      baby         public        Renegade626   1584933321   \n",
       "2      baby         public  mytrendybabystore   1584983015   \n",
       "3      baby         public  mytrendybabystore   1584988382   \n",
       "4      baby         public         kellimoxie   1585016982   \n",
       "\n",
       "  link_flair_text_color link_flair_type  retrieved_on  score  \\\n",
       "0                  dark            text    1584915676      1   \n",
       "1                  dark            text    1584933322      1   \n",
       "2                  dark            text    1584983020      1   \n",
       "3                  dark            text    1584988392      1   \n",
       "4                  dark            text    1585016988      1   \n",
       "\n",
       "   subreddit_subscribers subreddit_type.1  ... is_robot_indexable is_self  \\\n",
       "0                   6852           public  ...              False    True   \n",
       "1                   6855           public  ...               True    True   \n",
       "2                   6856           public  ...              False    True   \n",
       "3                   6857           public  ...              False    True   \n",
       "4                   6860           public  ...               True    True   \n",
       "\n",
       "  is_video locked  media_only  over_18  pinned  spoiler  stickied   timestamp  \n",
       "0    False  False       False    False   False    False     False  2020-03-22  \n",
       "1    False  False       False    False   False    False     False  2020-03-22  \n",
       "2    False  False       False    False   False    False     False  2020-03-23  \n",
       "3    False  False       False    False   False    False     False  2020-03-23  \n",
       "4    False  False       False    False   False    False     False  2020-03-23  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit                 object\n",
       "subreddit_type            object\n",
       "author                    object\n",
       "created_utc                int64\n",
       "link_flair_text_color     object\n",
       "link_flair_type           object\n",
       "retrieved_on               int64\n",
       "score                      int64\n",
       "subreddit_subscribers      int64\n",
       "subreddit_type.1          object\n",
       "title                     object\n",
       "domain                    object\n",
       "full_link                 object\n",
       "url                       object\n",
       "is_reddit_media_domain      bool\n",
       "no_follow                   bool\n",
       "send_replies                bool\n",
       "can_mod_post                bool\n",
       "contest_mode                bool\n",
       "is_crosspostable            bool\n",
       "is_meta                     bool\n",
       "is_original_content         bool\n",
       "is_robot_indexable          bool\n",
       "is_self                     bool\n",
       "is_video                    bool\n",
       "locked                      bool\n",
       "media_only                  bool\n",
       "over_18                     bool\n",
       "pinned                      bool\n",
       "spoiler                     bool\n",
       "stickied                    bool\n",
       "timestamp                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pets    2959\n",
       "baby    1724\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_data['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Binary y column based on subreddit name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type.1</th>\n",
       "      <th>...</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>locked</th>\n",
       "      <th>media_only</th>\n",
       "      <th>over_18</th>\n",
       "      <th>pinned</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>purehoopla</td>\n",
       "      <td>1584915670</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1584915676</td>\n",
       "      <td>1</td>\n",
       "      <td>6852</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>Renegade626</td>\n",
       "      <td>1584933321</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1584933322</td>\n",
       "      <td>1</td>\n",
       "      <td>6855</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit subreddit_type       author  created_utc link_flair_text_color  \\\n",
       "0      baby         public   purehoopla   1584915670                  dark   \n",
       "1      baby         public  Renegade626   1584933321                  dark   \n",
       "\n",
       "  link_flair_type  retrieved_on  score  subreddit_subscribers  \\\n",
       "0            text    1584915676      1                   6852   \n",
       "1            text    1584933322      1                   6855   \n",
       "\n",
       "  subreddit_type.1  ... is_self is_video locked media_only  over_18  pinned  \\\n",
       "0           public  ...    True    False  False      False    False   False   \n",
       "1           public  ...    True    False  False      False    False   False   \n",
       "\n",
       "   spoiler  stickied   timestamp  y  \n",
       "0    False     False  2020-03-22  1  \n",
       "1    False     False  2020-03-22  1  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_data['y'] = comb_data['subreddit'].map(lambda x: 1 if x == 'baby' else 0)\n",
    "\n",
    "comb_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.63186\n",
       "1    0.36814\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_data['y'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          What It’s Like to Be (Very) Pregnant Right Now\n",
       "1                       Tummy sleeper wakes several times\n",
       "2        Online Resources For Kids During Home Quarantine\n",
       "3               Chicken Pot Pie The Whole Family Can Make\n",
       "4       I could use advice regarding a cancelled baby ...\n",
       "                              ...                        \n",
       "4678                            Cat begs for food nonstop\n",
       "4679     Found a price sticker in Royal Canin Small Adult\n",
       "4680    6 month old kitty sometimes plays with mouth o...\n",
       "4681                     Remember to keep pets safe today\n",
       "4682                       Getting pee smell out of couch\n",
       "Name: title, Length: 4683, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_data['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let create a function to tokenize,Lemmatizing & Stemming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_to_clean):\n",
    "    # subs charact in the brackets\n",
    "    text_to_clean = re.sub( '[^a-zA-Z0-9]', ' ', text_to_clean)\n",
    "    # subs tabs,newlines and \"whitespace-like\"\n",
    "    text_to_clean = re.sub( '\\s+', ' ', text_to_clean).strip()\n",
    "    # convert to lowercase split indv words\n",
    "    words = text_to_clean.lower().split() \n",
    "    #converting stop words to set\n",
    "    stops = set(stopwords.words('english')) \n",
    "    # removing stop words\n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    #Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_data['clean_title'] = comb_data.apply(lambda x: clean_text(x['title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_data['clean_url']=comb_data.apply(lambda x: clean_text(x['url']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_data['clean_full_link']=comb_data.apply(lambda x: clean_text(x['full_link']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_data.drop_dublicates(subset = ['subreddit', 'subreddit_type'\n",
    "#                     'author', 'created_utc', 'link_flair_text_color','link_flair_type', 'retrieved_on', 'score'\n",
    "# 'subreddit_subscribers','subreddit_type.1','title', 'domain', 'full_link', 'url',\n",
    "# 'is_reddit_media_domain','no_follow', 'send_replies', 'can_mod_post'\n",
    "# 'contest_mode', 'is_crosspostable', 'is_meta', 'is_original_content'\n",
    "# 'is_robot_indexable', 'is_self', 'is_video', 'locked', 'media_only',\n",
    "# 'over_18', 'pinned', 'spoiler', 'stickied', 'timestamp'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type.1</th>\n",
       "      <th>...</th>\n",
       "      <th>media_only</th>\n",
       "      <th>over_18</th>\n",
       "      <th>pinned</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_url</th>\n",
       "      <th>clean_full_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>purehoopla</td>\n",
       "      <td>1584915670</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1584915676</td>\n",
       "      <td>1</td>\n",
       "      <td>6852</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>1</td>\n",
       "      <td>like pregnant right</td>\n",
       "      <td>https www reddit com r baby comments fn8hsd li...</td>\n",
       "      <td>https www reddit com r baby comments fn8hsd li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baby</td>\n",
       "      <td>public</td>\n",
       "      <td>Renegade626</td>\n",
       "      <td>1584933321</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>1584933322</td>\n",
       "      <td>1</td>\n",
       "      <td>6855</td>\n",
       "      <td>public</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>1</td>\n",
       "      <td>tummy sleeper wakes several times</td>\n",
       "      <td>https www reddit com r baby comments fncyve tu...</td>\n",
       "      <td>https www reddit com r baby comments fncyve tu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit subreddit_type       author  created_utc link_flair_text_color  \\\n",
       "0      baby         public   purehoopla   1584915670                  dark   \n",
       "1      baby         public  Renegade626   1584933321                  dark   \n",
       "\n",
       "  link_flair_type  retrieved_on  score  subreddit_subscribers  \\\n",
       "0            text    1584915676      1                   6852   \n",
       "1            text    1584933322      1                   6855   \n",
       "\n",
       "  subreddit_type.1  ... media_only over_18 pinned spoiler  stickied  \\\n",
       "0           public  ...      False   False  False   False     False   \n",
       "1           public  ...      False   False  False   False     False   \n",
       "\n",
       "    timestamp  y                        clean_title  \\\n",
       "0  2020-03-22  1                like pregnant right   \n",
       "1  2020-03-22  1  tummy sleeper wakes several times   \n",
       "\n",
       "                                           clean_url  \\\n",
       "0  https www reddit com r baby comments fn8hsd li...   \n",
       "1  https www reddit com r baby comments fncyve tu...   \n",
       "\n",
       "                                     clean_full_link  \n",
       "0  https www reddit com r baby comments fn8hsd li...  \n",
       "1  https www reddit com r baby comments fncyve tu...  \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "establish X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comb_data['clean_title']\n",
    "y = comb_data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at sklearn's stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'towards', 'another', 'several', 'whether', 'moreover', 'ten', 'cant', 'serious', 'though', 'nothing', 'forty', 'least', 'even', 'eg', 'up', 'thick', 'thereafter', 'them', 'eleven', 'across', 'often', 'un', 'being', 'any', 'am', 'due', 'should', 'whatever', 'there', 'no', 'something', 'nine', 'himself', 'bottom', 'without', 'per', 'which', 'our', 'de', 'onto', 'next', 'ourselves', 'had', 'those', 'about', 'some', 'via', 'each', 'mine', 'neither', 'yourselves', 'out', 'last', 'most', 'not', 'part', 'hundred', 'here', 'made', 'only', 'fill', 'find', 'but', 'once', 'between', 'itself', 'amoungst', 'when', 'be', 're', 'found', 'too', 'together', 'ever', 'mill', 'as', 'behind', 'somewhere', 'than', 'sixty', 'else', 'yours', 'still', 'also', 'beside', 'former', 'in', 'whence', 'thereby', 'whither', 'anyhow', 'of', 'its', 'then', 'everywhere', 'except', 'over', 'beyond', 'under', 'by', 'could', 'his', 'toward', 'fifteen', 'whenever', 'may', 'my', 'thru', 'for', 'will', 'whereby', 'own', 'wherever', 'ours', 'such', 'because', 'until', 'sometimes', 'co', 'we', 'is', 'already', 'five', 'again', 'seems', 'back', 'their', 'however', 'indeed', 'been', 'although', 'therefore', 'hereupon', 'so', 'one', 'a', 'after', 'either', 'against', 'has', 'someone', 'couldnt', 'below', 'herself', 'herein', 'to', 'him', 'seem', 'becoming', 'enough', 'would', 'whereafter', 'on', 'meanwhile', 'noone', 'perhaps', 'latterly', 'through', 'during', 'thin', 'seeming', 'every', 'that', 'they', 'into', 'become', 'hereafter', 'mostly', 'are', 'see', 'this', 'six', 'where', 'two', 'besides', 'almost', 'namely', 'off', 'these', 'hers', 'he', 'hereby', 'call', 'put', 'side', 'twelve', 'more', 'ie', 'same', 'the', 'it', 'done', 'above', 'among', 'take', 'me', 'seemed', 'whom', 'i', 'themselves', 'her', 'might', 'have', 'if', 'full', 'latter', 'became', 'front', 'empty', 'third', 'upon', 'others', 'along', 'many', 'while', 'nor', 'elsewhere', 'your', 'whereas', 'therein', 'bill', 'or', 'whose', 'yet', 'thereupon', 'was', 'thence', 'everything', 'throughout', 'show', 'whereupon', 'at', 'everyone', 'becomes', 'top', 'keep', 'otherwise', 'further', 'us', 'wherein', 'she', 'other', 'anyone', 'cry', 'first', 'with', 'you', 'amongst', 'beforehand', 'before', 'rather', 'four', 'an', 'move', 'myself', 'around', 'three', 'very', 'since', 'anywhere', 'con', 'alone', 'formerly', 'from', 'inc', 'cannot', 'always', 'must', 'hasnt', 'please', 'why', 'sincere', 'describe', 'give', 'and', 'nobody', 'afterwards', 'can', 'anything', 'hence', 'whole', 'what', 'get', 'well', 'whoever', 'nevertheless', 'somehow', 'now', 'less', 'name', 'do', 'how', 'eight', 'much', 'go', 'both', 'nowhere', 'few', 'interest', 'within', 'none', 'sometime', 'yourself', 'twenty', 'fire', 'fifty', 'system', 'never', 'were', 'ltd', 'amount', 'all', 'anyway', 'detail', 'thus', 'down', 'etc', 'who'})\n"
     ]
    }
   ],
   "source": [
    "print(CountVectorizer(stop_words = 'english').get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=['eg', 'un', 'am', 'ltd', 'etc', 'may', 'my', 'thru', 'for',\n",
    "                                'from', 'inc', 'con', 'more', 'ie', ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model\n",
    "\n",
    "Since this is a classification problem (and we'll be using accuracy as our metric), the baseline model is to predicted the most frequently occuring target class.The baseline accuracy is the percentage of the majority class, regardless of whether it is 1 or 0. It serves as the benchmark for our model to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.63186\n",
       "1    0.36814\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 1 as 'baby' and 0 as 'Pets' expect an accuracy of ~36% and ~63%. Any model performing well above this will be a significant improvement to the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.6319385140905209\n",
      "Train Score: 0.6318337129840547\n",
      "Cross Val Score: 0.6318599549389303\n"
     ]
    }
   ],
   "source": [
    "# instantiate DummyClassifier\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# score on test\n",
    "print('Test Score:', dummy.score(X_test, y_test))\n",
    "\n",
    "# score on train\n",
    "print('Train Score:', dummy.score(X_train, y_train))\n",
    "\n",
    "# score on cross val\n",
    "print('Cross Val Score:', cross_val_score(dummy, X, y, cv =5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model is at ~63% accuracy.\n",
    "Therefore, we are looking to build a model that does better than 63%, otherwise our best bet is to blindly guess the same outcome every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=['eg', 'un', 'am', 'ltd', 'etc', 'may', 'my', 'thru', 'for',\n",
    "                      'from', 'inc', 'con', 'more', 'ie', ])\n",
    "   \n",
    "tvect = TfidfVectorizer()\n",
    "lr = LogisticRegression()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates 2 Logistic Regression pipelines using count and tfidf vectorizers\n",
    "with two stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lr_pipe = Pipeline([\n",
    "    ('cv', cv), #transformer\n",
    "    ('lr', lr)  # estimator\n",
    "])\n",
    "tfidf_lr_pipe = Pipeline([\n",
    "    ('tvect', tvect), #transformer\n",
    "    ('lr', lr)        # estimator\n",
    "])\n",
    "\n",
    "# Creates two Naive Bayes pipelines with Multinomial and binomial NB\n",
    "# with two stages:\n",
    "\n",
    "m_nb_pipe = Pipeline([\n",
    "    ('cv', cv),    #transformer\n",
    "    ('mnb', mnb)   # estimator\n",
    "])\n",
    "b_nb_pipe = Pipeline([\n",
    "    ('tvect', tvect), #transformer  \n",
    "    ('bnb', bnb)      # estimator\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Search over the following values of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1 = {\n",
    "    'cv__max_features':[None,5_000,10_000],\n",
    "    #'cv__min_df': [2, 3],\n",
    "    #'cv__max_df': [.9, .95],\n",
    "    'cv__ngram_range':[(1,1),(1,2)]\n",
    "}\n",
    "params_2 = {\n",
    "    'tvect__max_features':[None,4_000,5_000,10_000],\n",
    "    #'tvect__min_df': [2, 3],\n",
    "    #'tvect__max_df': [.9, .95],\n",
    "    'tvect__ngram_range':[(1,1),(1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Instantiate GridSearchCV & Fit GridSearch to training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'cv__max_features': None, 'cv__ngram_range': (1, 2)}\n",
      "Best Score: 0.9447593342330185\n",
      "Test Score: 0.9538855678906917\n",
      "Train Score: 0.994874715261959\n"
     ]
    }
   ],
   "source": [
    "gs_cv = GridSearchCV(cv_lr_pipe, \n",
    "                  params_1,\n",
    "                  cv = 5)\n",
    "gs_cv.fit(X_train, y_train)\n",
    "print('Best parameters:', gs_cv.best_params_)\n",
    "print('Best Score:', gs_cv.best_score_)\n",
    "print('Test Score:', gs_cv.score(X_test, y_test))\n",
    "print('Train Score:', gs_cv.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'tvect__max_features': None, 'tvect__ngram_range': (1, 1)}\n",
      "Best Score: 0.9342212658002133\n",
      "Test Score: 0.9453458582408198\n",
      "Train Score: 0.9806378132118451\n"
     ]
    }
   ],
   "source": [
    "gs_tfidf = GridSearchCV(tfidf_lr_pipe, \n",
    "                  params_2,\n",
    "                  cv = 5)\n",
    "gs_tfidf.fit(X_train, y_train)\n",
    "print('Best parameters:', gs_tfidf.best_params_)\n",
    "print('Best Score:', gs_tfidf.best_score_)\n",
    "print('Test Score:', gs_tfidf.score(X_test, y_test))\n",
    "print('Train Score:', gs_tfidf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'cv__max_df': 0.9, 'cv__max_features': None, 'cv__min_df': 2, 'cv__ngram_range': (1, 1)}\n",
      "Best Score: 0.9245370876949824\n",
      "Test Score: 0.9291204099060631\n",
      "Train Score: 0.9607061503416856\n"
     ]
    }
   ],
   "source": [
    "gs_m_nb = GridSearchCV(m_nb_pipe, \n",
    "                  params_1,\n",
    "                  cv = 5)\n",
    "gs_m_nb.fit(X_train, y_train)\n",
    "print('Best parameters:', gs_m_nb.best_params_)\n",
    "print('Best Score:', gs_m_nb.best_score_)\n",
    "print('Test Score:', gs_m_nb.score(X_test, y_test))\n",
    "print('Train Score:', gs_m_nb.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'tvect__max_df': 0.9, 'tvect__max_features': None, 'tvect__min_df': 2, 'tvect__ngram_range': (1, 1)}\n",
      "Best Score: 0.9208370313633474\n",
      "Test Score: 0.9222886421861657\n",
      "Train Score: 0.9570045558086561\n"
     ]
    }
   ],
   "source": [
    "gs_b_nb = GridSearchCV(b_nb_pipe, \n",
    "                  params_2,\n",
    "                  cv = 5)\n",
    "gs_b_nb.fit(X_train, y_train)\n",
    "print('Best parameters:', gs_b_nb.best_params_)\n",
    "print('Best Score:', gs_b_nb.best_score_)\n",
    "print('Test Score:', gs_b_nb.score(X_test, y_test))\n",
    "print('Train Score:', gs_b_nb.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Score:  0.929947761526709\n",
      "Train Score:  0.9983\n",
      "Train Score:  0.9342\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "# Transform the corpus\n",
    "X_train_vec = cv.fit_transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "# an instance of RandomForestClassifier \n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fiting\n",
    "rf.fit(X_train_vec,y_train)\n",
    "\n",
    "#cross val score is the best_score of model\n",
    "print(\"Cross Val Score: \", cross_val_score(rf,X_train_vec,y_train,cv=5).mean())\n",
    "\n",
    "#rounded down features in each split for classification problem.\n",
    "print(\"Train Score: \", round(rf.score(X_train_vec,y_train),4))\n",
    "print(\"Train Score: \", round(rf.score(X_test_vec,y_test),4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Score:  0.929947761526709\n",
      "Train Score:  0.9983\n",
      "Test Score:  0.9223\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "# Transform the corpus\n",
    "X_train_vec = cv.fit_transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "# an instance of ExtraTreesClassifier\n",
    "et = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "# Fiting\n",
    "et.fit(X_train_vec,y_train)\n",
    "#cross val score is the best_score of model\n",
    "print(\"Cross Val Score: \",cross_val_score(rf,X_train_vec,y_train,cv=5).mean())\n",
    "\n",
    "#rounded down features in each split for classification problem.\n",
    "print(\"Train Score: \", round(et.score(X_train_vec,y_train),4))\n",
    "print(\"Test Score: \", round(et.score(X_test_vec,y_test),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baggin Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top Feature barh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiate StandardScalar\n",
    "# sc = StandardScaler()\n",
    "\n",
    "# # fit on X_train AND transform it\n",
    "# sc_X_train = sc.fit_transform(X_train)\n",
    "\n",
    "# # ONLY transform the X_test\n",
    "# sc_X_test = sc.transform(X_test)\n",
    "\n",
    "# # instantiate model\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# # fit model\n",
    "# knn.fit(sc_X_train, y_train)\n",
    "# # train score\n",
    "# print(\"Train Score: \", knn.score(sc_X_train, y_train))\n",
    "# # test score\n",
    "# print(\"Test Score: \", knn.score(sc_X_test, y_test))\n",
    "# # cross val\n",
    "# print(\"Cross Val Score: \", cross_val_score(knn, sc.transform(X), y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using accuracy as metric, evaluate all models on the training, testing sets and cross val score/best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                           | Train Score        | Test Score         | Cross Val Score/Best Score |\n",
    "|---------------------------------|--------------------|--------------------|----------------------------|\n",
    "| Baseline                        | 0.6318337129840547 | 0.6319385140905209 | 0.6318599549389303         |\n",
    "| CountVectorizer(estimator = lr) | 0.9812072892938497 | 0.9393680614859095 | 0.9382086539981277         |\n",
    "| TfidfVectorizer(estimator = lr) | 0.9783599088838268 | 0.9436379163108455 | 0.9302334723387355         |\n",
    "| MultinomialNB                   | 0.9607061503416856 | 0.9291204099060631 | 0.9245370876949824         |\n",
    "| BernoulliNB                     | 0.9570045558086561 | 0.9222886421861657 | 0.9208370313633474         |\n",
    "| Random Forest                   | 0.9342             | 0.9983             | 0.929947761526709          |\n",
    "| Extra Trees                     | 0.9223             | 0.9983             | 0.929947761526709          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best scoring model to evaluate\n",
    "predictions = ...predict(X_test)\n",
    "# check out confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert confusion matrix to dataframe\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    columns = ['predicted neg', 'predicted pos'],\n",
    "                    index = ['actual neg', 'actual pos'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model accuracy\n",
    "accuracy =(1096+1157)/(1096+153+87+1157)\n",
    "accuracy*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Pushshift API](https://github.com/pushshift/api)\n",
    "\n",
    "- https://api.pushshift.io/reddit/search/submission?subreddit=baby\n",
    "\n",
    "- https://api.pushshift.io/reddit/search/submission?subreddit=Pets\n",
    "\n",
    "- https://www.epochconverter.com\n",
    "\n",
    "- https://pushshift.io/api-parameters/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
